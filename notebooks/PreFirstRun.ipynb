{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import zipline\n",
    "import alphalens\n",
    "import pyfolio\n",
    "\n",
    "from zipline.data import bundles\n",
    "from zipline.pipeline import Pipeline\n",
    "from zipline.utils.calendars import get_calendar\n",
    "from zipline.pipeline.engine import SimplePipelineEngine\n",
    "from zipline.pipeline.factors import CustomFactor, SimpleBeta, DailyReturns, AverageDollarVolume\n",
    "\n",
    "from zipline.data.bundles import register\n",
    "from zipline.data.bundles.csvdir import csvdir_equities\n",
    "\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "from zipline.pipeline.loaders import USEquityPricingLoader\n",
    "from zipline.assets._assets import Equity\n",
    "from zipline.api import symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom Bundle for Loading the SEP daily stock dataset from Sharadar, from a dump.\n",
    "\n",
    "Created by Peter Harrington (pbharrin) on 3/8/18.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from zipline.utils.calendars import get_calendar\n",
    "import sys\n",
    "\n",
    "\n",
    "METADATA_HEADERS = ['start_date', 'end_date', 'auto_close_date',\n",
    "                    'symbol', 'exchange', 'asset_name']\n",
    "\n",
    "\n",
    "def check_for_abnormal_returns(df, thresh=3.0):\n",
    "    \"\"\"Checks to see if any days have abnormal returns\"\"\"\n",
    "    returns = df['close'].pct_change()\n",
    "    abnormal_rets = returns[returns > thresh]\n",
    "    if abnormal_rets.shape[0] > 0:\n",
    "        sys.stderr.write('Abnormal returns for: {}\\n'.format(df.ix[0]['ticker']))\n",
    "        sys.stderr.write('{}\\n'.format(str(abnormal_rets)))\n",
    "\n",
    "\n",
    "def from_sep_dump(file_name, start=None, end=None):\n",
    "    \"\"\"\n",
    "    ticker,date,open,high,low,close,volume,dividends,lastupdated\n",
    "    A,2008-01-02,36.67,36.8,36.12,36.3,1858900.0,0.0,2017-11-01\n",
    "\n",
    "    To use this make your ~/.zipline/extension.py look similar this:\n",
    "\n",
    "    from zipline.data.bundles import register\n",
    "    from alphacompiler.data.loaders.sep_quandl import from_sep_dump\n",
    "\n",
    "    register(\"sep\",\n",
    "         from_sep_dump(\"/path/to/your/SEP/dump/SHARADAR_SEP_69.csv\"),)\n",
    "\n",
    "    \"\"\"\n",
    "    us_calendar = get_calendar(\"NYSE\").all_sessions\n",
    "    ticker2sid_map = {}\n",
    "\n",
    "    def ingest(environ,\n",
    "               asset_db_writer,\n",
    "               minute_bar_writer,  # unused\n",
    "               daily_bar_writer,\n",
    "               adjustment_writer,\n",
    "               calendar,\n",
    "               cache,\n",
    "               show_progress,\n",
    "               output_dir,\n",
    "               # pass these as defaults to make them 'nonlocal' in py2\n",
    "               start=start,\n",
    "               end=end):\n",
    "\n",
    "        print(\"starting ingesting data from: {}\".format(file_name))\n",
    "\n",
    "        # read in the whole dump (will require ~7GB of RAM)\n",
    "        df = pd.read_csv(file_name, index_col='date',\n",
    "                         parse_dates=['date'], na_values=['NA'])\n",
    "\n",
    "        # drop unused columns, dividends will be used later\n",
    "        df = df.drop(['lastupdated', 'dividends', 'closeunadj'], axis=1)\n",
    "\n",
    "        # counter of valid securites, this will be our primary key\n",
    "        sec_counter = 0\n",
    "        data_list = []  # list to send to daily_bar_writer\n",
    "        metadata_list = []  # list to send to asset_db_writer (metadata)\n",
    "\n",
    "        # iterate over all the unique securities and pack data, and metadata\n",
    "        # for writing\n",
    "        for tkr, df_tkr in df.groupby('ticker'):\n",
    "            df_tkr = df_tkr.sort_index()\n",
    "\n",
    "            row0 = df_tkr.ix[0]  # get metadata from row\n",
    "\n",
    "            print(\" preparing {}\".format(row0[\"ticker\"]))\n",
    "            check_for_abnormal_returns(df_tkr)\n",
    "\n",
    "            # check to see if there are missing dates in the middle\n",
    "            this_cal = us_calendar[(us_calendar >= df_tkr.index[0]) & (us_calendar <= df_tkr.index[-1])]\n",
    "            if len(this_cal) != df_tkr.shape[0]:\n",
    "                print('MISSING interstitial dates for: %s using forward fill' % row0[\"ticker\"])\n",
    "                print('number of dates missing: {}'.format(len(this_cal) - df_tkr.shape[0]))\n",
    "                df_desired = pd.DataFrame(index=this_cal.tz_localize(None))\n",
    "                df_desired = df_desired.join(df_tkr)\n",
    "                df_tkr = df_desired.fillna(method='ffill')\n",
    "\n",
    "            # update metadata; 'start_date', 'end_date', 'auto_close_date',\n",
    "            # 'symbol', 'exchange', 'asset_name'\n",
    "            metadata_list.append((df_tkr.index[0],\n",
    "                                  df_tkr.index[-1],\n",
    "                                  df_tkr.index[-1] + pd.Timedelta(days=1),\n",
    "                                  row0[\"ticker\"],\n",
    "                                  \"SEP\",  # all have exchange = SEP\n",
    "                                  row0[\"ticker\"]  # TODO: can we delete this?\n",
    "                                  )\n",
    "                                 )\n",
    "\n",
    "            # drop metadata columns\n",
    "            df_tkr = df_tkr.drop(['ticker'], axis=1)\n",
    "\n",
    "            # pack data to be written by daily_bar_writer\n",
    "            data_list.append((sec_counter, df_tkr))\n",
    "            ticker2sid_map[tkr] = sec_counter  # record the sid for use later\n",
    "            sec_counter += 1\n",
    "\n",
    "        print(\"writing data for {} securities\".format(len(metadata_list)))\n",
    "        daily_bar_writer.write(data_list, show_progress=False)\n",
    "\n",
    "        # write metadata\n",
    "        asset_db_writer.write(equities=pd.DataFrame(metadata_list,\n",
    "                                                    columns=METADATA_HEADERS))\n",
    "        print(\"a total of {} securities were loaded into this bundle\".format(\n",
    "            sec_counter))\n",
    "\n",
    "        # read in Dividend History\n",
    "        dfd = pd.read_csv(file_name, index_col='date',\n",
    "                         parse_dates=['date'], na_values=['NA'])\n",
    "        # drop rows where dividends == 0.0\n",
    "        dfd = dfd[dfd[\"dividends\"] != 0.0]\n",
    "        dfd = dfd.dropna()\n",
    "\n",
    "        dfd.loc[:, 'ex_date'] = dfd.loc[:, 'record_date'] = dfd.index\n",
    "        dfd.loc[:, 'declared_date'] = dfd.loc[:, 'pay_date'] = dfd.index\n",
    "        dfd.loc[:, 'sid'] = dfd.loc[:, 'ticker'].apply(lambda x: ticker2sid_map[x])\n",
    "        dfd = dfd.rename(columns={'dividends': 'amount'})\n",
    "        dfd = dfd.drop(['open', 'high', 'low', 'close', 'volume', 'lastupdated', 'ticker', 'closeunadj'], axis=1)\n",
    "\n",
    "        # # format dfd to have sid\n",
    "        adjustment_writer.write(dividends=dfd)\n",
    "\n",
    "    return ingest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Equity(8554 [SPY])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipline.assets.Equity(8554,'NYSE',symbol='SPY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline import CustomFactor, Pipeline  \n",
    "from zipline.pipeline.data import USEquityPricing  \n",
    "from zipline.pipeline.engine import PipelineEngine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from zipline.pipeline.engine import SimplePipelineEngine\n",
    "  # Required for USEquityPricing\n",
    "from zipline.pipeline import Pipeline\n",
    "\n",
    "def register_data(start_date, end_date, bundle_name, address):\n",
    "\n",
    "    start_session = pd.Timestamp(start_date, tz='utc')\n",
    "    end_session = pd.Timestamp(end_date, tz='utc')\n",
    "\n",
    "    register(bundle_name, csvdir_equities(['daily'],address,),\n",
    "    calendar_name='NYSE', start_session=start_session,\n",
    "    end_session=end_session)\n",
    "\n",
    "\n",
    "class PricingLoader(object):\n",
    "    def __init__(self, bundle_data):\n",
    "        self.loader = USEquityPricingLoader(\n",
    "            bundle_data.equity_daily_bar_reader,\n",
    "            bundle_data.adjustment_reader)\n",
    "\n",
    "    def get_loader(self, column):\n",
    "        if column not in USEquityPricing.columns:\n",
    "            raise Exception('Column not in USEquityPricing')\n",
    "        return self.loader\n",
    "\n",
    "def build_pipeline_engine(bundle_data, trading_calendar):\n",
    "    pricing_loader = PricingLoader(bundle_data)\n",
    "\n",
    "    engine = SimplePipelineEngine(\n",
    "        get_loader=pricing_loader.get_loader,\n",
    "        calendar=trading_calendar.all_sessions,\n",
    "        asset_finder=bundle_data.asset_finder)\n",
    "\n",
    "    return engine\n",
    "\n",
    "# Loading stock list from file\n",
    "def stock_list(file_name):\n",
    "    all_stocks = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            # remove linebreak which is the last character of the string\n",
    "            currentPlace = line[:-1]\n",
    "            # add item to the list\n",
    "            all_stocks.append(currentPlace)\n",
    "        return all_stocks\n",
    "\n",
    "def get_universe_tickers(engine, universe, end_date):\n",
    "    universe_end_date = pd.Timestamp(end_date, tz='UTC')\n",
    "\n",
    "    universe_tickers = engine \\\n",
    "        .run_pipeline(\n",
    "        Pipeline(screen=universe),\n",
    "        universe_end_date,\n",
    "        universe_end_date) \\\n",
    "        .index.get_level_values(1) \\\n",
    "        .values.tolist()\n",
    "\n",
    "    return universe_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = AverageDollarVolume(window_length=120).top(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_extensions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-518e3951963b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mload_extensions\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menviron\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_extensions' is not defined"
     ]
    }
   ],
   "source": [
    "import os \n",
    "load_extensions( default=True, extensions=[], strict=True, environ=os.environ, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on curry in module zipline.data.bundles.core:\n",
      "\n",
      "register(name='__no__default__', f='__no__default__', calendar_name='NYSE', start_session=None, end_session=None, minutes_per_day=390, create_writers=True)\n",
      "    Register a data bundle ingest function.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        The name of the bundle.\n",
      "    f : callable\n",
      "        The ingest function. This function will be passed:\n",
      "    \n",
      "          environ : mapping\n",
      "              The environment this is being run with.\n",
      "          asset_db_writer : AssetDBWriter\n",
      "              The asset db writer to write into.\n",
      "          minute_bar_writer : BcolzMinuteBarWriter\n",
      "              The minute bar writer to write into.\n",
      "          daily_bar_writer : BcolzDailyBarWriter\n",
      "              The daily bar writer to write into.\n",
      "          adjustment_writer : SQLiteAdjustmentWriter\n",
      "              The adjustment db writer to write into.\n",
      "          calendar : trading_calendars.TradingCalendar\n",
      "              The trading calendar to ingest for.\n",
      "          start_session : pd.Timestamp\n",
      "              The first session of data to ingest.\n",
      "          end_session : pd.Timestamp\n",
      "              The last session of data to ingest.\n",
      "          cache : DataFrameCache\n",
      "              A mapping object to temporarily store dataframes.\n",
      "              This should be used to cache intermediates in case the load\n",
      "              fails. This will be automatically cleaned up after a\n",
      "              successful load.\n",
      "          show_progress : bool\n",
      "              Show the progress for the current load where possible.\n",
      "    calendar_name : str, optional\n",
      "        The name of a calendar used to align bundle data.\n",
      "        Default is 'NYSE'.\n",
      "    start_session : pd.Timestamp, optional\n",
      "        The first session for which we want data. If not provided,\n",
      "        or if the date lies outside the range supported by the\n",
      "        calendar, the first_session of the calendar is used.\n",
      "    end_session : pd.Timestamp, optional\n",
      "        The last session for which we want data. If not provided,\n",
      "        or if the date lies outside the range supported by the\n",
      "        calendar, the last_session of the calendar is used.\n",
      "    minutes_per_day : int, optional\n",
      "        The number of minutes in each normal trading day.\n",
      "    create_writers : bool, optional\n",
      "        Should the ingest machinery create the writers for the ingest\n",
      "        function. This can be disabled as an optimization for cases where\n",
      "        they are not needed, like the ``quantopian-quandl`` bundle.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This function my be used as a decorator, for example:\n",
      "    \n",
      "    .. code-block:: python\n",
      "    \n",
      "       @register('quandl')\n",
      "       def quandl_ingest_function(...):\n",
      "           ...\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    zipline.data.bundles.bundles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bundles.register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lalopey/opt/anaconda3/envs/zipline/lib/python3.5/site-packages/ipykernel_launcher.py:4: UserWarning: Overwriting bundle with name 'sep'\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "universe = AverageDollarVolume(window_length=120).top(1500)\n",
    "trading_calendar = get_calendar('NYSE') \n",
    "ingest_func = bundles.csvdir.csvdir_equities(['daily'], 'sep')\n",
    "bundles.register('sep', from_sep_dump('.'))\n",
    "bundle_data = bundles.load('sep')\n",
    "engine = build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(factors, universe):\n",
    "    factors_pipe = OrderedDict()\n",
    "        \n",
    "    for name, f in factors.items():\n",
    "        factors_pipe[name] = f\n",
    "                    \n",
    "    pipe = Pipeline(screen=universe, columns=factors_pipe)\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "\n",
    "def make_factors():\n",
    "    \n",
    "    def BetaToSPY(): \n",
    "        return SimpleBeta(target=Equity(8554,'NYSE'),regression_length=260) \n",
    "\n",
    "    class AverageMarketReturn(CustomFactor):\n",
    "            inputs = [DailyReturns()]\n",
    "            window_length = 1\n",
    "            window_safe = True\n",
    "            mask = universe\n",
    "\n",
    "            def compute(self, today, assets, out, returns):\n",
    "                # returns are days in rows, assets across columns\n",
    "                out[:] = np.nanmean(returns)\n",
    "\n",
    "    def Market_style():\n",
    "        return BetaToSPY() * AverageMarketReturn()\n",
    "    \n",
    "    #def Market_style():\n",
    "    #    return DailyReturns()\n",
    "    \n",
    "    all_factors = {\n",
    "        'Market_style': Market_style(),\n",
    "    }\n",
    "    \n",
    "    return all_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_start_date = pd.Timestamp('2014-01-03', tz='UTC')\n",
    "universe_end_date = pd.Timestamp('2020-02-03', tz='UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Equity(32 [AAPL]),\n",
       " Equity(240 [ADBE]),\n",
       " Equity(770 [AMD]),\n",
       " Equity(877 [AMZN]),\n",
       " Equity(1465 [AVGO]),\n",
       " Equity(1590 [BA]),\n",
       " Equity(1593 [BABA]),\n",
       " Equity(1595 [BAC]),\n",
       " Equity(1996 [BKNG]),\n",
       " Equity(2092 [BMY]),\n",
       " Equity(2244 [BRK.B]),\n",
       " Equity(2442 [BYND]),\n",
       " Equity(2456 [C]),\n",
       " Equity(3345 [CMCSA]),\n",
       " Equity(3772 [CRM]),\n",
       " Equity(3848 [CSCO]),\n",
       " Equity(4073 [CVX]),\n",
       " Equity(4378 [DHR]),\n",
       " Equity(4412 [DIS]),\n",
       " Equity(5536 [FB]),\n",
       " Equity(6316 [GE]),\n",
       " Equity(6642 [GOOG]),\n",
       " Equity(6643 [GOOGL]),\n",
       " Equity(7027 [HD]),\n",
       " Equity(7971 [INTC]),\n",
       " Equity(8342 [JNJ]),\n",
       " Equity(8364 [JPM]),\n",
       " Equity(9283 [MA]),\n",
       " Equity(9433 [MCD]),\n",
       " Equity(10021 [MRK]),\n",
       " Equity(10085 [MSFT]),\n",
       " Equity(10196 [MU]),\n",
       " Equity(10511 [NFLX]),\n",
       " Equity(10700 [NOW]),\n",
       " Equity(10915 [NVDA]),\n",
       " Equity(11726 [PFE]),\n",
       " Equity(11758 [PG]),\n",
       " Equity(12471 [PYPL]),\n",
       " Equity(12499 [QCOM]),\n",
       " Equity(13021 [ROKU]),\n",
       " Equity(13674 [SHOP]),\n",
       " Equity(14647 [T]),\n",
       " Equity(15389 [TSLA]),\n",
       " Equity(15584 [UBER]),\n",
       " Equity(15712 [UNH]),\n",
       " Equity(15875 [V]),\n",
       " Equity(16282 [VZ]),\n",
       " Equity(16411 [WFC]),\n",
       " Equity(16557 [WMT]),\n",
       " Equity(16799 [XOM])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe_tickers = engine\\\n",
    "    .run_pipeline(\n",
    "        Pipeline(screen=universe),\n",
    "        universe_end_date,\n",
    "        universe_end_date)\\\n",
    "    .index.get_level_values(1)\\\n",
    "    .values.tolist()\n",
    "    \n",
    "universe_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NonExistentAssetInTimeFrame",
     "evalue": "The target asset 'Equity(8554)' does not exist for the entire timeframe between 2012-12-21 00:00:00+00:00 and 2020-02-03 00:00:00+00:00.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNonExistentAssetInTimeFrame\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-dad145cfc9cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muniverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfactor_start_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniverse_end_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/zipline/lib/python3.5/site-packages/zipline/pipeline/engine.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, start_date, end_date)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0massets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0minitial_workspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         )\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/zipline/lib/python3.5/site-packages/zipline/pipeline/engine.py\u001b[0m in \u001b[0;36mcompute_chunk\u001b[0;34m(self, graph, dates, assets, initial_workspace)\u001b[0m\n\u001b[1;32m    535\u001b[0m                     \u001b[0mmask_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                     \u001b[0massets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m                 )\n\u001b[1;32m    539\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/zipline/lib/python3.5/site-packages/zipline/pipeline/filters/filter.py\u001b[0m in \u001b[0;36m_compute\u001b[0;34m(self, arrays, dates, assets, mask)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_my_asset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             raise NonExistentAssetInTimeFrame(\n\u001b[0;32m--> 535\u001b[0;31m                 \u001b[0masset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m             )\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNonExistentAssetInTimeFrame\u001b[0m: The target asset 'Equity(8554)' does not exist for the entire timeframe between 2012-12-21 00:00:00+00:00 and 2020-02-03 00:00:00+00:00."
     ]
    }
   ],
   "source": [
    "pipe= make_pipeline(make_factors(),universe=universe)\n",
    "engine.run_pipeline(pipeline=pipe, start_date=factor_start_date, end_date=universe_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_data = bundles.load('quantopian-quandl')\n",
    "trading_calendar = get_calendar('NYSE')\n",
    "engine = build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Equity(8554)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Equity(8554,'NYSE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
